{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/momo/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import jiwer\n",
    "import torch\n",
    "import tiktoken\n",
    "import matplotlib\n",
    "import seaborn\n",
    "from transformers import pipeline\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Speech-to-Text (ใช้ Whisper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"biodatlab/whisper-th-medium-combined\"\n",
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=MODEL_NAME,\n",
    "    chunk_length_s=30,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/momo/Library/Python/3.9/lib/python/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed language=<|th|>, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of language=<|th|>.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "audio_file = \"/Users/momo/Downloads/Telesales.wav\"\n",
    "transcription = pipe(\n",
    "    audio_file,\n",
    "    generate_kwargs={\"language\": \"<|th|>\", \"return_timestamps\": False}, \n",
    ")[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Transcription\n",
    "with open(\"transcription.txt\", \"w\", encoding=\"utf-8\") as txt_file:\n",
    "    txt_file.write(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save as CSV\n",
    "# csv_filename = \"transcription1.csv\"\n",
    "# with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\"Transcription\"])\n",
    "#     writer.writerow([transcription])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def thai_to_arabic(text):\n",
    "    thai_num_dict = {\n",
    "        \"ศูนย์\": 0, \"หนึ่ง\": 1, \"สอง\": 2, \"สาม\": 3, \"สี่\": 4, \"ห้า\": 5,\n",
    "        \"หก\": 6, \"เจ็ด\": 7, \"แปด\": 8, \"เก้า\": 9, \"สิบ\": 10, \"ยี่สิบ\": 20,\n",
    "        \"ร้อย\": 100, \"พัน\": 1000, \"หมื่น\": 10000, \"แสน\": 100000, \"ล้าน\": 1000000\n",
    "    }\n",
    "    \n",
    "    def convert_number(match):\n",
    "        words = match.group().strip().split()\n",
    "        total = 0\n",
    "        temp = 0\n",
    "\n",
    "        for word in words:\n",
    "            if word in thai_num_dict:\n",
    "                num = thai_num_dict[word]\n",
    "                if num == 10 and temp == 0:  # กรณี \"สิบ\"\n",
    "                    temp = 10\n",
    "                elif num == 10 and temp > 0:  # กรณี \"สามสิบ\"\n",
    "                    temp *= 10\n",
    "                elif num >= 100:  # กรณี \"ร้อย พัน หมื่น\"\n",
    "                    temp = (temp or 1) * num\n",
    "                    total += temp\n",
    "                    temp = 0\n",
    "                else:\n",
    "                    temp += num\n",
    "            else:\n",
    "                total += temp\n",
    "                temp = 0\n",
    "\n",
    "        total += temp\n",
    "        return str(total)\n",
    "\n",
    "    return re.sub(r\"((?:ศูนย์|หนึ่ง|สอง|สาม|สี่|ห้า|หก|เจ็ด|แปด|เก้า|สิบ|ยี่สิบ|ร้อย|พัน|หมื่น|แสน|ล้าน)+)\", convert_number, text)\n",
    "\n",
    "# อ่าน transcription.txt\n",
    "input_file = \"transcription.txt\"\n",
    "output_file = \"transcription_cleaned.txt\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    transcription = file.read().strip()\n",
    "\n",
    "# แปลงตัวเลขไทยเป็นเลขอารบิก\n",
    "transcription_cleaned = thai_to_arabic(transcription)\n",
    "\n",
    "# บันทึกผลลัพธ์ลงไฟล์ใหม่\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(transcription_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# อ่านข้อมูลจากไฟล์ .txt\n",
    "input_file = \"transcription.txt\"\n",
    "output_file = \"result_sentiment.txt\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    transcription = file.read().strip()\n",
    "\n",
    "# วิเคราะห์ Sentiment ของข้อความ\n",
    "sentiment_result = sentiment_model(transcription)\n",
    "\n",
    "# บันทึกผลลัพธ์ลงไฟล์ใหม่\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(str(sentiment_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4298e94f7f754f2b83c0e8e18fc79b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a258a836439e4bfe9700849eff8a5270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2db60e0e1e48908057f2686da12b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70115b22bfc4489a1ba82a1b72d1093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73b14a927b74a56863c3d2d99ca71ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7e73fe41c246debe7229ca727aeb4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'anger', 'score': 0.2928387224674225}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained multilingual emotion detection model\n",
    "emotion_model = pipeline('text-classification', model=\"bhadresh-savani/bert-base-uncased-emotion\")\n",
    "\n",
    "# Example transcription\n",
    "transcription = \"รู้สึกเครียดมากเลยค่ะ เพราะการประชุมที่ยาวนานและไม่สามารถตัดสินใจได้\"\n",
    "\n",
    "# Get emotions\n",
    "result = emotion_model(transcription)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Evaluate Speech Recognition (WER & CER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcription.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    ground_truth = file.read().strip()  # ลบช่องว่าง\n",
    "\n",
    "with open(\"text.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    predicted_text = file.read().strip()  # ลบช่องว่าง\n",
    "\n",
    "def evaluate_transcription(ground_truth, predicted_text):\n",
    "    wer = jiwer.wer(ground_truth, predicted_text)\n",
    "    cer = jiwer.cer(ground_truth, predicted_text)\n",
    "    return {\"WER\": wer, \"CER\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Recognition Evaluation: {'WER': 0.109375, 'CER': 0.004794520547945206}\n"
     ]
    }
   ],
   "source": [
    "# ประเมินคุณภาพของ Speech Recognition\n",
    "eval_result = evaluate_transcription(ground_truth, transcription)\n",
    "print(\"Speech Recognition Evaluation:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,    \n",
    "    chunk_overlap=200,  \n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(transcription)\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9m/8xrxscqd5dd6xk22wk581jlr0000gn/T/ipykernel_13650/3766608019.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  temp_db = Chroma(collection_name=\"langchain\",\n"
     ]
    }
   ],
   "source": [
    "#load from disk\n",
    "persist_directory = 'chroma_db/'\n",
    "temp_db = Chroma(collection_name=\"langchain\",\n",
    "                 persist_directory=persist_directory, \n",
    "             embedding_function=HuggingFaceEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_db._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: LLM for Sales Coaching & Feature Extraction (ใช้ Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9m/8xrxscqd5dd6xk22wk581jlr0000gn/T/ipykernel_13650/2160584547.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"mistral\")  #\"mixtral\"\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "llm = Ollama(model=\"mistral\")  #\"mixtral\"\n",
    "\n",
    "retrieval_prompt = \"\"\"\n",
    "You are a professional sales trainer specializing in insurance sales coaching.  \n",
    "- Your task is to provide **clear, structured, and actionable** advice based on only the retrieved documents. \n",
    "<Response Guidelines>\n",
    "- Use three sentences maximum and keep the answer concise\n",
    "- You MUST NOT mention something like \"according to the document\" or \"context\" in the answer.\n",
    "- You MUST answer in English if the question contains all English. You MUST answer in Thai if the question contains Thai.\n",
    "</Response Guidelines>\n",
    "<Query>\n",
    "{question}\n",
    "</Query>\n",
    "<Context>\n",
    "{context}\n",
    "</Context>\n",
    "<Answer>\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(retrieval_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\n",
    "        \"context\": lambda query: temp_db.similarity_search(query, k=5),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Feature Extraction Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction Evaluation: {'Relevant Features Found': 2, 'Total Features Expected': 3}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_feature_extraction(transcription, extracted_features):\n",
    "    relevant_keywords = [\"เงินคืน\", \"รับประกัน\", \"การันตี\"]\n",
    "    extracted_count = sum(1 for word in relevant_keywords if word in extracted_features.lower())\n",
    "    return {\"Relevant Features Found\": extracted_count, \"Total Features Expected\": len(relevant_keywords)}\n",
    "\n",
    "extracted_features = \"เงินคืนการันตีทุกปี 24 ปี\"\n",
    "feature_eval = evaluate_feature_extraction(transcription, extracted_features)\n",
    "print(\"Feature Extraction Evaluation:\", feature_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Sales Coaching Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sales_coaching(question, expected_response):\n",
    "    response = llm.invoke(question)\n",
    "    similarity = jiwer.wer(expected_response, response)\n",
    "    return {\"WER Similarity Score\": similarity, \"LLM Response\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sales_coaching(question, expected_responses):\n",
    "    response = llm.invoke(question)\n",
    "    \n",
    "    transform = jiwer.Compose([\n",
    "        jiwer.ToLowerCase(),\n",
    "        jiwer.RemovePunctuation(),\n",
    "        jiwer.RemoveMultipleSpaces(),\n",
    "        jiwer.Strip()\n",
    "    ])\n",
    "    \n",
    "    llm_response_clean = transform(response)\n",
    "    wer_scores = [jiwer.wer(transform(resp), llm_response_clean) for resp in expected_responses]\n",
    "    \n",
    "    return {\"WER Similarity Score\": min(wer_scores), \"LLM Response\": response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"จะเชิญชวนให้ลูกค้าซื้อกรมธรรม์ได้อย่างไร?\"\n",
    "expected_responses = [\n",
    "    \"คุณสามารถเน้นถึงประโยชน์ของกรมธรรม์ เช่น ความคุ้มครอง การลดหย่อนภาษี และผลตอบแทนที่มั่นคง\",\n",
    "    \"คุณควรอธิบายถึงผลประโยชน์ของกรมธรรม์ที่เหมาะกับลูกค้า\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Coaching Evaluation: {'WER Similarity Score': 4.0, 'LLM Response': 'ขอแสดงความชี่ใจที่ทำตัวเอง, สิ่งที่สามารถทำให้ลูกค้าซื้อคำนำหน้าได้อย่างเห็นแบบดีเป็นคำสั่งของ 3 ข้อ:\\n\\n1. แสดงความเชื่อถือ: ลูกค้าอ่านติดทุกสิ่งในรีวิวและคำนำหน้าที่ขายไว้มักจะชี้ว่าเป็นสิ่งที่ดีที่สุด. สิ่งสำคัญที่ต้องทำคือเชื่อถือให้ลูกค้าเห็นว่าคุณแสดงความเชื่อถือสิ่งที่ขายไว้ และจะพูดตรงเชิญความตั้งใจของลูกค้า\\n\\n2. ประสานด้วยสถานหมาย: ทำให้ลูกค้ารู้ว่าจุดประสงค์ของสิ่งที่ขายไว้. แสดงสิ่งที่สนใจเพื่อช่วยลูกค้าพบสิ่งที่เห็นว่าเป็นมักจะต้องการ\\n\\n3. โปรไฟล์ที่ดี: มีข้อมูลที่คุณสามารถแสดงติดลบเหตุผลที่ทำให้ลูกค้าควรซื้อคำนำหน้า. สามารถทำให้แสดงตัวตนของคุณ และแสดงความเชื่อถือกับสิ่งที่ขายไว้ได้อย่างสม่ำเสมอ.\\n\\nขอให้พร้อมๆ ทำตัวเอง!'}\n"
     ]
    }
   ],
   "source": [
    "sales_eval = evaluate_sales_coaching(question, expected_responses)\n",
    "print(\"Sales Coaching Evaluation:\", sales_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
